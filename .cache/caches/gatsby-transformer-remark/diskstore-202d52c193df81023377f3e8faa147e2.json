{"expireTime":9007201009626692000,"key":"transformer-remark-markdown-html-55cac056a0482ebc6801349a10463c9b-gatsby-remark-external-linksgatsby-remark-imagesgatsby-remark-code-titlesgatsby-remark-prismjs-","val":"<ul>\n<li>Built <strong>large-scale healthcare data pipelines</strong> processing <strong>20M+ records per day</strong> for geocoding member locations using <strong>Cascading Java, Hadoop, AWS, and Redshift</strong>.</li>\n<li>Reimplemented the same workflows in <strong>PySpark and Pig</strong> to benchmark and optimize performance across different big data technologies.</li>\n<li>Developed the <strong>PCP Attribution module</strong>, a core healthcare system that processes millions of records to determine accurate <strong>NPI values</strong> for clients.</li>\n<li>Created a <strong>Point-of-Service (POS) Winner Buffer</strong> module to identify <strong>eligible members for insurance</strong> plans based on specific criteria.</li>\n<li>Designed workflows to <strong>convert large Parquet files to CSV (and vice versa)</strong> for downstream analytics and reporting.</li>\n<li>Utilized <strong>AWS S3 for data storage</strong> and <strong>Amazon Redshift for querying and analytics</strong>, improving data accessibility and processing efficiency.</li>\n</ul>"}